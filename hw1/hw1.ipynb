{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hw1 b05601005 陳廷安\n",
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Solved Optimum: \n",
      "[ 4.32e-09]\n",
      "[ 7.04e-01]\n",
      "[ 7.04e-01]\n",
      "[ 8.89e-01]\n",
      "[ 2.59e-01]\n",
      "[ 2.59e-01]\n",
      "[ 5.27e-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "\n",
    "P = np.array([[0,0,0], \n",
    "              [0,1,0],\n",
    "              [0,0,1]])\n",
    "\n",
    "q = np.array([0,0,0]).T\n",
    "\n",
    "g1 = np.array([-1,4,0])\n",
    "g2 = np.array([-1,1,3])\n",
    "g3 = np.array([-1,1,-1])\n",
    "g4 = np.array([1,0,0])\n",
    "g5 = np.array([1,2,-5])\n",
    "g6 = np.array([1,2,3])\n",
    "g7 = np.array([1,2,3])\n",
    "\n",
    "G = -np.vstack((g1,g2,g3,g4,g5,g6,g7))\n",
    "\n",
    "h = -np.ones((7,1))\n",
    "\n",
    "P = matrix(P, tc='d')\n",
    "q = matrix(q, tc='d')\n",
    "G = matrix(G, tc='d')\n",
    "h = matrix(h, tc='d')\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Solved Optimum: \\n\", sol['x'], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPrklEQVR4nO3de5CddXnA8e+zS3azC6HBJogk0cUqDohUpgtWqUW5FW/YOjJyEXHARkEUpyoWQkXUWgodlI6MmCqjCHIZLjqIHS5TlKkIuKHcYlAoF7loWeSey242+/SPHCAkm+Tsnvfsm9/u9zOTmZz3HH7v807Cd968e855IzORJJWro+4BJEmtMeSSVDhDLkmFM+SSVDhDLkmF26qOnc6ZMyf7+vrq2LUkFWvJkiVPZObc9bfXEvK+vj4GBgbq2LUkFSsiHhpru5dWJKlwhlySCmfIJalwhlySCmfIJalwlYU8Ijoj4n8i4idVrSlJ2rwqz8hPAJZVuJ4kqQmVhDwi5gPvAb5TxXrSVHbaVUs57aqldY+hKaSqDwR9AzgRmLWxF0TEQmAhwKtf/eqKdiuV59ePPVv3CJpiWj4jj4j3Ao9n5pJNvS4zF2dmf2b2z527wSdMJUkTVMWllb2BgyPiQeBiYN+IuKCCdSVJTWg55Jl5UmbOz8w+4FDgvzLzwy1PJklqiu8jl6TCVfrth5n5M+BnVa4pSdo0z8glqXCGXJIKZ8glqXCGXJIKZ8glqXCGXJIKZ8glqXCGXJIKZ8glqXCGXJIKV+lH9CVJGxodvguWfxsYhp7D6Zj5jkrXN+SS1EajT38RVl380oahnzE64y/o+NOLKtuHl1YkqU1GR+5/ecRfsHoJoyuvrGw/hlyS2uX5TdzGePn5le3GkEtS4Qy5JLXLNsds/LneIyrbjSGXpDbp2OrPYOYHN3xiq93p6B1j+wT5rhVJaqOO2V9jdPgQWH4u5DD0HEZHz4GV7sOQS1KbdXTtAV3fbt/6bVtZkjQpDLkkFc6QS1LhDLkkFc6QS1LhDLkkFc6QS1LhDLkkFc6QS1LhDLkkFc6QS1LhDLkkFa7lkEfEgoi4ISKWRcTSiDihisEkSc2p4tsPR4DPZuZtETELWBIR12XmrytYW5K0GS2fkWfm7zPztsbvnwOWAfNaXVeS1JxKr5FHRB+wB3DLGM8tjIiBiBgYHByscreSNK1VFvKI2Aa4HPhMZj67/vOZuTgz+zOzf+7cuVXtVpKmvUpCHhEzWBvxCzPziirWlCQ1p4p3rQTwXWBZZp7V+kiSpPGo4ox8b+BIYN+IuL3x690VrCtJakLLbz/MzP8GooJZJEkT4Cc7JalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfINe1lJledew1H9B3Le3oP59NvO5mlN/2m7rGkphlyTXsXfPUyFn/+Bzz+uycYXrWaZTffyxcO/Aq/XfK/dY8mNcWQa1obXjXMpWf8mFXLh16+feUQ3z/10pqmksbHkGtae+LRJyE2/PLOTLj/jgcnfyBpAgy5prXtdpjN6JrRMZ+bv/OOkzyNNDGGXNNaz9Yzed/HD6C7t/tl27t7uzjy1ENqmkoan5ZvLCGV7u/PPJKZ28zkirOvZmj5ENu/Zi6fPPtodv/rXeseTWqKIde019nZyUe/fChHnfYhVg+P0NU9o+6RpHHx0orUEBFGXEUy5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUrKuSrVgzxwF0P8ewfn6t7FGmLlZnkyP3kyMN1j6JJUsmXZkXEQcDZQCfwncw8vYp113XJGT/igq9cRnR0MLJ6hL3/di8+f95xdM3sqnpXUrFyeIB8+h9g9Bkgyc4FxHbfJLbaqe7R1EYtn5FHRCdwDvAuYFfgsIio9Ps/b7j4F/zgy5exavkQK59byepVq7npR7dy9nH/UeVupKLlmkHyqY/B6B+AlcAqWHMf+eThZA7XPZ7aqIpLK3sB92Xm/bn2b8vFwPsrWPdFF/3LFQytWO+eiqtWc8NFv2Dl8lVV7koqVq68EnLN+lshV8HQjbXMpMlRRcjnAetejHukse1lImJhRAxExMDg4OC4dvDU/z095vaOjmD508vHtZY0Za15DBjacHuugdHHJ30cTZ4qQr7hnWshN9iQuTgz+zOzf+7cuePawW5770J0bLibnlk9vOJV241rLWmqiu63QPSO/eSMPSZ3GE2qKkL+CLBgncfzgccqWPdFR3/tMGZuPZOOzpfG7e7t4thvfJSOjqLeeCO1T/f+0NkHrHv/0R7o3oeYsUtNQ2kyVPGulV8Br4+InYBHgUOBwytY90UL3jCPby35Vy786uUs/cU97PDaV3L4SR/gz9/xxip3IxUtYga84ofkiu/ByqsgZkDPoUTvh+oeTW3WcsgzcyQijgeuYe3bD8/LzKUtT7aeea97FSd+7/iql5WmlOjoJbY5DrY5ru5RNIkqeR95Zv4U+GkVa0mSxscLzJJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUOEMuSYUz5JJUuJZCHhFnRsQ9EXFnRFwZEbOrGkyS1JxWz8ivA3bLzN2B3wIntT6SJGk8Wgp5Zl6bmSONhzcD81sfSZI0HlVeIz8a+M+NPRkRCyNiICIGBgcHK9ytJE1vW23uBRFxPbDDGE8tyswfN16zCBgBLtzYOpm5GFgM0N/fnxOaVpK0gc2GPDP339TzEXEU8F5gv8w00JI0yTYb8k2JiIOALwD7ZOaKakaSJI1Hq9fIvwnMAq6LiNsj4twKZpIkjUNLZ+SZ+bqqBpEkTYyf7JSkwhlySSqcIZekwhlySSqcIZekwhlySSqcIZekwhlySSqcIZekwhlySSqcIZeAGy/7JR/f43N88JXHcMrBp/PAXQ/VPZLUNEOuae+Kf7+aMz56Dvff8RDPDD7LrVcv4dNvO4UHlz5c92hSUwy5prXVw6v5/j9dwtCKoRe3ZcLQiiHO/9IlNU4mNc+Qa1obfPiPjI6ObrA9M1l28701TCSNnyHXtDZ7+z9hzZoNQw7wyr65kzyNNDGGXNNa76we9j/i7XT3dL1se3dvF0ec8sGappLGp6UbS0hTwafO+RidW3Vy7fk/B6Bn624W/ttH2PNv3lzzZFJzDLmmvRldMzjhWwv5xFlH8fzTK5i9/bZ0dnbWPZbUNEMuNXT3dNPd0133GNK4eY1ckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpXScgj4nMRkRExp4r1JEnNaznkEbEAOAD4XevjSJLGq4oz8q8DJwJZwVqSpHFqKeQRcTDwaGbe0cRrF0bEQEQMDA4OtrJbSdI6Nvt95BFxPbDDGE8tAk4GDmxmR5m5GFgM0N/f79m7JFVksyHPzP3H2h4RbwJ2Au6ICID5wG0RsVdm/qHSKSVJGzXhOwRl5l3A9i88jogHgf7MfKKCuSRJTfJ95JJUuMru2ZmZfVWtJUlqnmfkklQ4Qy5JhTPkklQ4Qy5JhTPkklQ4Qy5JhTPkklQ4Qy5JhTPkklQ4Qy5JhavsI/qSpLFlDsHQTcBq6Hor0TGr0vUNuSS1UQ7fSj71iRceQY6Q255GR+8HKtuHl1YkqU1ydDn51ELI5xu/lgND8OyXyJEHKtuPIZekdhm6AYgxnhghV15Z2W4MuSS1Sy6HHB3jiREYfb6y3RhySWqX7r8Cxgp5DzFzv8p2Y8glqU2icx5sfQzQw4uXWKJ3beC73lbZfnzXiiS1Ucesz5Dde5MrL4McJma+B7r3pXHT+koYcklqs+jak+jas23re2lFkgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcIZckgpnyCWpcC2HPCI+FRG/iYilEXFGFUNJkprX0rcfRsQ7gfcDu2fmUERsX81YkqRmtXpGfixwemYOAWTm462PJEkaj1ZDvjPw9oi4JSJ+HhEb/cLdiFgYEQMRMTA4ONjibqVy7brjtuy647Z1j6EpZLOXViLiemCHMZ5a1PjvtwP+EtgTuDQiXpuZuf6LM3MxsBigv79/g+el6eLU972x7hE0xWw25Jm5/8aei4hjgSsa4b41IkaBOYCn3JI0SVq9tPIjYF+AiNgZ6AKeaHUoSVLzWr1n53nAeRFxNzAMHDXWZRVJUvu0FPLMHAY+XNEskqQJ8JOdklQ4Qy5JhTPkklQ4Qy5JhYs63mQSEYPAQy0sMYep8TbHqXIcMHWOZaocB3gsW6JWj+M1mTl3/Y21hLxVETGQmf11z9GqqXIcMHWOZaocB3gsW6J2HYeXViSpcIZckgpXasgX1z1ARabKccDUOZapchzgsWyJ2nIcRV4jlyS9pNQzcklSgyGXpMIVHfKI+FxEZETMqXuWiYqIr0TEnRFxe0RcGxE71j3TREXEmRFxT+N4royI2XXPNBERcUjjZuKjEVHcW94i4qDGDdHvi4h/rHueiYqI8yLi8ca3qxYtIhZExA0Rsazxd+uEKtcvNuQRsQA4APhd3bO06MzM3D0z3wz8BPhi3QO14Dpgt8zcHfgtcFLN80zU3cAHgBvrHmS8IqITOAd4F7ArcFhE7FrvVBP2PeCguoeoyAjw2czchbV3VPtklX8uxYYc+DpwIlD0T2sz89l1Hm5NwceTmddm5kjj4c3A/DrnmajMXJaZv6l7jgnaC7gvM+9vfM30xcD7a55pQjLzRuDJuueoQmb+PjNva/z+OWAZMK+q9Vu9sUQtIuJg4NHMvCMi6h6nZRHxz8BHgGeAd9Y8TlWOBi6pe4hpaB7w8DqPHwHeUtMsGkNE9AF7ALdUteYWG/LN3PT5ZODAyZ1o4jZ1LJn548xcBCyKiJOA44FTJ3XAcdjcsTRes4i1/5S8cDJnG49mjqNQY53ZFPuvvKkmIrYBLgc+s96/xluyxYZ8Yzd9jog3ATsBL5yNzwdui4i9MvMPkzhi0zZ1A+v1/BC4mi045Js7log4CngvsN+WfNu/cfyZlOYRYME6j+cDj9U0i9YRETNYG/ELM/OKKtfeYkO+MZl5F7D9C48j4kGgPzOL/Ga0iHh9Zt7beHgwcE+d87QiIg4CvgDsk5kr6p5nmvoV8PqI2Al4FDgUOLzekRRrzzq/CyzLzLOqXr/kH3ZOFadHxN0RcSdrLxdV+rakSfZNYBZwXePtlOfWPdBERMTfRcQjwFuBqyPimrpnalbjh83HA9ew9gdql2bm0nqnmpiIuAj4JfCGiHgkIo6pe6YW7A0cCezb+H/j9oh4d1WL+xF9SSqcZ+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVLj/B8glx377M25cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [[1,0], [0,1], [0,-1], [-1,0], [0,2], [0,-2], [-2,0]]\n",
    "y = [-1,-1,-1,1,1,1,1]\n",
    "\n",
    "def zTrans(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    z1 = x2**2 - 2*x1 -2\n",
    "    z2 = x1**2 - 2*x2 -1\n",
    "    return [z1,z2]\n",
    "\n",
    "z = [zTrans(xx) for xx in x]\n",
    "z1, z2 = zip(*z)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(z1, z2, c=y)\n",
    "plt.plot([-0.5,-0.5], [-6,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "By the solution given above, we have $b=1$, $w_1=2$ and $w_2\\approx0$. <br />\n",
    "Hence, the hyperplane can be written as $2z_1 + 1 = 0$, <br />\n",
    "that is, $2x_2^2 - 4x_1 - 3= 0. (eq.1)$\n",
    "<br /><br /><br /><br /><br /><br /><br /><br />\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Solved Optimum: \n",
      "[ 4.32e-09]\n",
      "[ 7.04e-01]\n",
      "[ 7.04e-01]\n",
      "[ 8.89e-01]\n",
      "[ 2.59e-01]\n",
      "[ 2.59e-01]\n",
      "[ 5.27e-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = [[1,0], [0,1], [0,-1], [-1,0], [0,2], [0,-2], [-2,0]]\n",
    "x = np.array(x)\n",
    "y = [-1,-1,-1,1,1,1,1]\n",
    "\n",
    "P = np.zeros([7,7])\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        P[i,j] = y[i]*y[j]*(1+sum(x[i]*x[j]))**2\n",
    "\n",
    "q = -1*np.ones((7,1))\n",
    "\n",
    "A = np.array(y).reshape(1,7)\n",
    "c = 0\n",
    "\n",
    "G = -np.identity(7)\n",
    "\n",
    "h = np.zeros((7,1))\n",
    "\n",
    "P = matrix(P, tc='d')\n",
    "q = matrix(q, tc='d')\n",
    "G = matrix(G, tc='d')\n",
    "h = matrix(h, tc='d')\n",
    "A = matrix(A, tc='d')\n",
    "c = matrix(c, tc='d')\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Solved Optimum: \\n\", sol['x'], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "By the solution given above, <br />\n",
    "we have $a_1=a_7\\approx 0, a_2=a_3=0.704, a_4=0.889, a_5=a_6=0.259$. <br />\n",
    "Since the support vector's $a \\neq 0$, <br />\n",
    "the support vectors are $x_2, x_3, x_4, x_5, x_6$.\n",
    "<br /><br /><br /><br /><br /><br />\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "<font size=\"4\"> \n",
    "By the lectures, <br />\n",
    "\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "g_{svm}(x) &=sign (\\sum_{sv} a_n y_n K(x_n, x)+b) \\\\\n",
    " & = sign(0.704\\times-1\\times(1+0x_1+1x_2)^2+ \\\\\n",
    " & 0.704\\times-1\\times(1+0x_1+-1x_2)^2+ \\\\\n",
    " & 0.889\\times1\\times(1+-1x_1+0x_2)^2+ \\\\\n",
    " & 0.259\\times1\\times(1+0x_1+2x_2)^2+ \\\\\n",
    " & 0.259\\times1\\times(1+0x_1+-2x_2)^2+ \\\\\n",
    " & 1.666\\\\\n",
    " & = sign(0.664x_2^2 + 0.889x_1^2 - 1.778x_1 + 1.665)\\\\\n",
    " & = sign(eq.3))\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "<font size=\"4\"> \n",
    "$eq.1=2x_2^2 - 4x_1 - 3= 0$ <br />\n",
    "$eq.3=0.664x_2^2 + 0.889x_1^2 - 1.778x_1 - 1.665$ <br />\n",
    "<br />\n",
    "By comparing the formulae, eq.1 and eq.3 are different. <br />\n",
    "Furthermore, they should not be the same as well, since they have different support vectors and different kernels.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "<font size=\"4\"> \n",
    "$L((b,w,\\epsilon),(\\alpha, \\beta))=\n",
    "\\frac{1}{2}\\ ww^T+C\\cdot \\sum_{n=1}^{N}\\epsilon_n\n",
    "\\ +\\ \\sum_{n=1}^{N}\\alpha_n\\cdot(\\rho_n-\\epsilon_n-y_n(w^tz_n+b))\n",
    "\\ +\\ \\sum_{n=1}^{N}\\beta_n\\cdot(-\\epsilon_n) $\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "<font size=\"4\"> \n",
    "$max_{\\alpha_n\\geq0, \\beta_n\\geq0}${ $min_{\\beta, w, \\epsilon}${ $L((b,w,\\epsilon),(\\alpha, \\beta))$ } } <br />\n",
    "\n",
    "Considering $\\frac{\\partial L}{\\partial \\epsilon}=0$, we can solve the problem without lossing optimality <br />\n",
    "if solving with implicit constraint $\\beta_n=C-\\alpha_n$ and explicity constraint $0\\leq\\alpha_n\\leq C$. <br />\n",
    "We can simplify the Lagrange dual problem to: <br />\n",
    "<br />\n",
    "$max_{0\\leq\\alpha_n\\leq C, \\beta_n=C-\\alpha_n}${ $min_{\\beta,w,\\epsilon}${ \n",
    "$\\frac{1}{2}w^Tw\\ +\\sum_{n=1}^{N}\\alpha_n\\cdot(\\rho_n-y_n(w^tz_n+b))$ \n",
    "} } <br />\n",
    "<br />\n",
    "For the inner problem <br />\n",
    "considering $\\frac{\\partial L}{\\partial b}=0$ and $\\frac{\\partial L}{\\partial w}=0$, <br />\n",
    "we can solve the problem without lossing optimality <br />\n",
    "if solving with implicit constraint $\\sum_{n=1}^{N}\\alpha_ny_n=0$ and $w_i=\\sum_{n=1}^{N}\\alpha_ny_nz_n$ <br />\n",
    "<br />\n",
    "We can simplify the Lagrange dual problem to: <br />\n",
    "<br />\n",
    "$max_{0\\leq\\alpha_n\\leq C, \\beta_n=C-\\alpha_n}$ { \n",
    "$-\\frac{1}{2}||\\sum_{n=1}^{N}\\alpha_ny_nz_n||^2 +\\sum_{n=1}^{N}\\rho_n\\alpha_n$ }, <br />\n",
    "<br />\n",
    "which is <br />\n",
    "$min_{\\alpha}\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}\\alpha_n\\alpha_my_ny_mz_n^Tz_m -\\sum_{n=1}^{N}\\rho_n\\alpha_n$ <br />\n",
    "<br />\n",
    "subject to <br />\n",
    "$\\sum_{n=1}^{N}\\alpha_ny_n=0$; <br />\n",
    "$\\alpha_n \\geq 0$, for $n=1,2,...,N$\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "<font size=\"4\"> \n",
    "This is the P_1^{'} SVM with $\\rho_n=0.5$<br />\n",
    "<br />\n",
    "$(P_1^{'})\\ min_{w^{'},\\beta^{'},\\epsilon^{'}} \\ \\ \\frac{1}{2}\\ w^{'}w^{'T}+C\\cdot \\sum_{n=1}^{N}\\epsilon^{'}_n$ <br />\n",
    "s.t. $y_n(w^{'T}x_n+b^{'})\\geq0.5-\\epsilon^{'}_n$ <br />\n",
    "<br />\n",
    "By scaling the objective function by 4 and constraint by 2 and we can have: <br />\n",
    "<br />\n",
    "$(P_1^{'})\\ min_{w^{'},\\beta^{'},\\epsilon^{'}} \\ \\ \\frac{1}{2}\\ 4w^{'}w^{'T}+2C\\cdot \\sum_{n=1}^{N}2\\epsilon^{'}_n$ <br />\n",
    "s.t. $y_n(2w^{'T}x_n+2b^{'})\\geq1-2\\epsilon^{'}_n$ <br />\n",
    "<br />\n",
    "It can also be represent as the eqaution below: <br />\n",
    "<br />\n",
    "$min_{w,\\beta,\\epsilon}\\ \\ \\frac{1}{2}\\ ww^T+2C\\cdot \\sum_{n=1}^{N}\\epsilon_n$ <br />\n",
    "s.t. $y_n(w^Tx+b)\\geq1-\\epsilon_n$ <br />\n",
    "<br />\n",
    "where $w=2w^{'}$, $b_n=2b^{'}$ and $\\epsilon_n=2\\epsilon^{'}_n$ <br />\n",
    "<br />\n",
    "Hence, Assume that $(\\beta_*^{'},w_*^{'})$ is the optimal solution of solving $P_1^{'}$ with all $\\rho_n=0.5$. \n",
    "<br />\n",
    "The optimal solution of $P_1$ with $C_1=2C$ can be express as $(\\beta_*,w_*)=(2\\beta_*^{'},2w_*^{'})$.\n",
    "<br /><br /><br /><br /><br /><br />\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "<font size=\"4\"> \n",
    "Soft-margin SVM dual is almost the same as hard-margin's, except that soft-margin SVM dual has a upper-bound $C$ for each $\\alpha_n$. Hence, if $C\\geq max_{1\\leq n\\leq N}a_n^*$, we can add the constraint $0\\leq a_n\\leq C$ to hard-margin SVM dual while not affecting the optimum result. As a result, this hard-margin SVM dual becomes a soft-margin SVM dual, having the same optimun $a*$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "<font size=\"4\"> \n",
    "Let the GRAM matrix of $K$ and $K_1$ be $M$ and $M_1$ respectively. <bt />\n",
    "</font>\n",
    "## (a)\n",
    "<font size=\"4\"> \n",
    "$K=(1-K_1(x,x^`))^{1}$ is not a valid kernel. <br />\n",
    "Here is a Counterexample: <br />\n",
    "<br />\n",
    "Let $M_1=\\begin{bmatrix}\n",
    "0.9 & 0.1 \\\\\n",
    "0.1 & 0.9 \n",
    "\\end{bmatrix}$\n",
    ", then\n",
    "$M= \\begin{bmatrix}\n",
    "0.1 & 0.9 \\\\\n",
    "0.9 & 0.1 \n",
    "\\end{bmatrix}$. <br />\n",
    "Since $det(M)<0$, $M$ is not a p.s.d matrix, therefore $K$ is not a valid kernel.\n",
    "</font>\n",
    "## (b)\n",
    "<font size=\"4\"> \n",
    "$K=(1-K_1(x,x^`))^{0}$ is a valid kernel. <br />\n",
    "Since $M=J$, and $J$ is a p.s.d. matrix.\n",
    "</font>\n",
    "## (c)\n",
    "<font size=\"4\"> \n",
    "$K=(1-K_1(x,x^`))^{-1}$ is a valid kernel. <br />\n",
    "<br />\n",
    "By Taylor expansion, \n",
    "$K=\\sum_{i=0}^{\\infty}(K_1(x,x^`))^{i}$ <br />\n",
    "<br />\n",
    "Let $K_n(x,x^`)$ and $K_m(x,x^`)$ be valid kernels <br /> \n",
    "<br /> \n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "K_n(x,x^`)\\times K_m(x,x^`) \n",
    " & = \\phi_n(x)^T\\phi_n(x^`)\\phi_m(x)^T\\phi_m(x) \\\\\n",
    " & = \\sum_{i=1}^n\\phi^i_n(x)\\phi^i_n(x^`)\\sum_{i=1}^n\\phi^j_m(x)\\phi^j_m(x)\\\\\n",
    " & = \\sum_{i=1}^n\\sum_{j=1}^n\\phi^i_n(x)\\phi^i_n(x^`)\\phi^j_m(x)\\phi^j_m(x)\\\\\n",
    " & = \\sum_{i=1}^n\\sum_{j=1}^n(\\phi^i_n(x)\\phi^j_m(x))(\\phi^i_n(x^`)\\phi^j_m(x))\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "<br /> \n",
    "Let $\\Phi^i(x)=\\sum_{j=1}^n\\phi_1^i(x)\\phi_2^j(x)$<br /> \n",
    "<br /> \n",
    "$\\Phi(x)=\\sum_{i=1}^n\\Phi^i(x)$<br /> \n",
    "$(\\Phi^i(x))^T(\\Phi^i(x))=\\sum_{j=1}^n\\phi_1^i(x)\\phi_2^j(x)\\phi_1^i(x)\\phi_2^j(x)$<br /> \n",
    "\n",
    "$\\begin{equation} \\label{eq2}\n",
    "\\begin{split}\n",
    "(\\Phi(x))^T(\\Phi(x)) \n",
    " & = \\sum_{i=1}^n(\\Phi^i(x))^T(\\Phi^i(x)) \\\\\n",
    " & = \\sum^n_{i=1}\\sum_{j=1}^n\\phi_1^i(x)\\phi_2^j(x)\\phi_1^i(x)\\phi_2^j(x)\\\\\n",
    " & = K_n(x,x^`)\\times K_m(x,x^`)\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "<br />  \n",
    "Hence, $K_n(x,x^`)\\times K_m(x,x^`)$ is a valid kernel.\n",
    "<br /> \n",
    "Also, the sum of valid kernels is also a valid kernel since the sum of p.s.d. matrices is still a psd matix.<br />\n",
    "<br />\n",
    "Hence, $K=\\sum_{i=0}^{\\infty}(K_1(x,x^`))^{i}$ is a valid kernel.\n",
    "</font> \n",
    "## (d)\n",
    "<font size=\"4\"> \n",
    "$K=(1-K_1(x,x^`))^{-2}$ is a valid kernel. <br />\n",
    "<br />\n",
    "Since $(1-K_1(x,x^`))^{-1}$ is a valid kernel, $(1-K_1(x,x^`))^{-2}=(1-K_1(x,x^`))^{-1}\\times(1-K_1(x,x^`))^{-1}$ is also a valid kernel.<br />\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "<font size=\"4\"> \n",
    "This is the SVM dual of kernel $K$ and parameter $C$. <br />\n",
    "<br />\n",
    "$min_{\\alpha}\\ \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}\\alpha_n\\alpha_my_ny_mK(x_n,x_m) -\\sum_{n=1}^{N}\\alpha_n$ <br />\n",
    "s.t. <br />\n",
    "$\\sum_{n=1}^{N}\\alpha_ny_n=0$; <br />\n",
    "$0\\leq\\alpha_n\\leq C$, for $n=1,2,...,N$ <br />\n",
    "<br />\n",
    "This is the SVM dual of kernel $pK$ and parameter $C/p$. <br />\n",
    "<br />\n",
    "$min_{\\alpha^*}\\ \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}\\alpha^*_n\\alpha^*_my_ny_mpK(x_n,x_m) -\\sum_{n=1}^{N}\\alpha^*_n$ <br />\n",
    "s.t. <br />\n",
    "$\\sum_{n=1}^{N}\\alpha^*_ny_n=0$; <br />\n",
    "$0\\leq\\alpha^*_n\\leq C/p$, for $n=1,2,...,N$ <br />\n",
    "<br />\n",
    "    \n",
    "The SVM dual of kernel $pK$ and parameter $C/p$ can also be written as:<br />\n",
    "    \n",
    "$min_{\\alpha^*}\\ \\frac{1}{2}\\frac{1}{p}(\\sum_{n=1}^{N}\\sum_{m=1}^{N}p\\alpha^*_np\\alpha^*_my_ny_mK(x_n,x_m) -\\sum_{n=1}^{N}p\\alpha^*_n)$ \n",
    "<br />\n",
    "$0\\leq p\\alpha^*_n\\leq C$, for $n=1,2,...,N$ <br />\n",
    "<br />\n",
    "\n",
    "Let $\\alpha^{**} = p\\alpha^*$. <br />\n",
    "    \n",
    "$min_{\\alpha^{**}}\\ \\frac{1}{2}\\frac{1}{p}(\\sum_{n=1}^{N}\\sum_{m=1}^{N}\\alpha^{**}_n\\alpha^{**}_my_ny_mK(x_n,x_m) -\\sum_{n=1}^{N}\\alpha^{**}_n)$ \n",
    "<br />\n",
    "s.t. <br />\n",
    "$\\sum_{n=1}^{N}\\alpha^{**}_ny_n=0$; <br />\n",
    "$0\\leq \\alpha^{**}_n\\leq C$, for $n=1,2,...,N$ <br />\n",
    "\n",
    "Since $p$ is a constant, we can ignore it for our objective function and find this dual problem is the same as that of kernel $K$ with parameter $C$. Hence, the two dual problems share the same optimal solution, that is, $\\alpha_{optim} = \\alpha^{**}_{optim} = p\\alpha^*_{optim}.$ <br />\n",
    "\n",
    "We first get the solution of $b^*$ by $b$ <br />\n",
    "<br />\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "b^* & = y_m - \\sum_{n=1}^{N} a^*_n y_n K^*(x_n, x) \\\\\n",
    " & = y_m - \\sum_{n=1}^{N} \\frac{\\alpha_n}{p} y_n pK^(x_n, x) \\\\\n",
    " & = b\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "\n",
    "(each $y_m$'s corresponding $a_m\\neq0$)<br />\n",
    "<br />\n",
    "We can show the two SVM has the same classifier.<br />\n",
    "<br />\n",
    "$\\begin{equation} \\label{eq2}\n",
    "\\begin{split}\n",
    "g^*(x) & = sign(\\sum_{n=1}^{N} a^*_n y_n K^*(x_n, x)+b*) \\\\\n",
    " & = sign(\\sum_{n=1}^{N} \\frac{\\alpha_n}{p} y_n pK(x_n, x)+b) \\\\\n",
    " & = g(x)\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11\n",
    "<font size=\"4\"> \n",
    "Load in data...\n",
    "<font />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from libsvm.svmutil import *\n",
    "file_train = \"./features.train.txt\"\n",
    "file_test = \"./features.test.txt\"\n",
    "x_train = []\n",
    "y_test = []\n",
    "\n",
    "with open(file_train, 'r', newline='\\n') as f:\n",
    "    rows = csv.reader(f, delimiter=' ')\n",
    "    train = [[float(ele) for ele in row if ele!=''] for row in rows]\n",
    "    \n",
    "with open(file_test, 'r', newline='\\n') as f:\n",
    "    rows = csv.reader(f, delimiter=' ')\n",
    "    test = [[float(ele) for ele in row if ele!=''] for row in rows]\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(train)\n",
    "x_train = train[:,1:]\n",
    "x_test = test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0% (0/4) (classification)\n",
      "Accuracy = 0% (0/4) (classification)\n",
      "Accuracy = 25% (1/4) (classification)\n",
      "Accuracy = 25% (1/4) (classification)\n",
      "Accuracy = 25% (1/4) (classification)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANR0lEQVR4nO3dfYxsdX3H8fen91YtPhSQ1VAwXTCGlhiruG1RE5pIVQQjNqEJJCpVm/tHg7VNTXsNiRr/oo9pja32FqnYGjBSG0mN6A3W0CaIXZAiDyKPVZTCWlo1NqlSvv1jz6XLunt3dubsLt+971cymZnf/M6c7+/MzGfPnDnnbKoKSVI/P7bTBUiSpmOAS1JTBrgkNWWAS1JTBrgkNbV3O2d23HHH1fz8/HbOUpLau/HGG79dVXOr27c1wOfn51lcXNzOWUpSe0n+ba12N6FIUlMGuCQ1ZYBLUlMGuCQ1ZYBLUlMGuCQ1ZYBLUlMGuCQ1ZYBLUlPbeiSmJG3G/P5P73QJo7n/knNGf07XwCWpKdfApScx10B1OK6BS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTGwZ4ksuSPJzk1hVtf5jkq0luSfL3SY7e2jIlSatNsgb+EeCsVW0HgRdW1YuArwHvGrkuSdIGNgzwqroOeGRV2+eq6tHh7heBE7egNknSYYyxDfytwGfWezDJviSLSRaXlpZGmJ0kCWYM8CQXA48CH1uvT1UdqKqFqlqYm5ubZXaSpBWm/ocOSS4EXgecWVU1XkmSpElMFeBJzgJ+D/ilqvrvcUuSJE1ikt0IrwCuB05J8kCStwEfAJ4JHExyc5IPbXGdkqRVNlwDr6oL1mj+8BbUIknaBI/ElKSmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmNgzwJJcleTjJrSvajk1yMMldw/UxW1umJGm1SdbAPwKctaptP3BtVb0AuHa4L0naRhsGeFVdBzyyqvlc4PLh9uXAG0auS5K0gWm3gT+3qh4EGK6fs17HJPuSLCZZXFpamnJ2kqTVtvxHzKo6UFULVbUwNze31bOTpCPGtAH+UJLjAYbrh8crSZI0iWkD/GrgwuH2hcCnxilHkjSpSXYjvAK4HjglyQNJ3gZcArwqyV3Aq4b7kqRttHejDlV1wToPnTlyLZKkTfBITElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKYMcElqygCXpKZmCvAkv53ktiS3JrkiydPGKkySdHhTB3iSE4DfBBaq6oXAHuD8sQqTJB3erJtQ9gI/kWQvcBTwrdlLkiRNYuoAr6pvAn8EfB14EPhOVX1udb8k+5IsJllcWlqavlJJ0hPMsgnlGOBc4CTgp4CnJ3nj6n5VdaCqFqpqYW5ubvpKJUlPMMsmlF8G7quqpar6IfBJ4OXjlCVJ2sgsAf514PQkRyUJcCZwxzhlSZI2Mss28BuAq4CbgK8Mz3VgpLokSRvYO8vEVfUe4D0j1SJJ2gSPxJSkpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpgxwSWrKAJekpmYK8CRHJ7kqyVeT3JHkZWMVJkk6vL0zTv9nwDVVdV6SpwBHjVCTJGkCUwd4kmcBZwC/BlBVPwB+ME5ZkqSNzLIJ5WRgCfjrJF9OcmmSp6/ulGRfksUki0tLSzPMTpK00iwBvhc4DfhgVb0E+D6wf3WnqjpQVQtVtTA3NzfD7CRJK80S4A8AD1TVDcP9q1gOdEnSNpg6wKvq34FvJDllaDoTuH2UqiRJG5p1L5S3Ax8b9kC5F3jL7CVJkiYxU4BX1c3Awki1SJI2wSMxJakpA1ySmpp1G7i05eb3f3qnSxjN/Zecs9MlaBdxDVySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmjLAJakpA1ySmpo5wJPsSfLlJP8wRkGSpMmMsQb+DuCOEZ5HkrQJMwV4khOBc4BLxylHkjSpWdfA/xT4XeCxEWqRJG3C1AGe5HXAw1V14wb99iVZTLK4tLQ07ewkSavMsgb+CuD1Se4HrgRemeRvV3eqqgNVtVBVC3NzczPMTpK00tQBXlXvqqoTq2oeOB/4fFW9cbTKJEmH5X7gktTU3jGepKq+AHxhjOeSJE3GNXBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJasoAl6SmDHBJamrqAE/yvCT/mOSOJLcleceYhUmSDm/vDNM+CvxOVd2U5JnAjUkOVtXtI9UmSTqMqdfAq+rBqrppuP094A7ghLEKkyQd3ijbwJPMAy8BbljjsX1JFpMsLi0tjTE7SRIjBHiSZwB/B/xWVX139eNVdaCqFqpqYW5ubtbZSZIGMwV4kh9nObw/VlWfHKckSdIkZtkLJcCHgTuq6k/GK0mSNIlZ1sBfAbwJeGWSm4fL2SPVJUnawNS7EVbVPwMZsRZJ0iZ4JKYkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNWWAS1JTBrgkNTVTgCc5K8mdSe5Osn+soiRJG5s6wJPsAf4ceC1wKnBBklPHKkySdHh7Z5j2F4C7q+pegCRXAucCt49RmJ5ofv+nd7qEUdx/yTk7XYK0a6SqppswOQ84q6p+fbj/JuAXq+qiVf32AfuGu6cAd05f7rY4Dvj2ThexQxz7ketIHn+Hsf90Vc2tbpxlDTxrtP3IX4OqOgAcmGE+2yrJYlUt7HQdO8GxH5ljhyN7/J3HPsuPmA8Az1tx/0TgW7OVI0ma1CwB/i/AC5KclOQpwPnA1eOUJUnayNSbUKrq0SQXAZ8F9gCXVdVto1W2c9ps7tkCjv3IdSSPv+3Yp/4RU5K0szwSU5KaMsAlqaldG+BJLkvycJJbp5j2pUm+Mpwi4P1JMrS/N8k3k9w8XM4ev/JxbHSagyRPTfLx4fEbksyveOxdQ/udSV6zon3qZfpkMcFyOSPJTUkeHY51aG2t1yzJsUkOJrlruD5mnWkvHPrcleTC7at6czYzxix7//D635LktBXTbDjeSZfdtqmqXXkBzgBOA26dYtovAS9jeV/3zwCvHdrfC7xzp8c2Qf17gHuAk4GnAP8KnLqqz28AHxpunw98fLh96tD/qcBJw/PsmXWZPhkuEy6XeeBFwEeB83a65hHG/COvGfAHwP7h9n7g99eY7ljg3uH6mOH2MTs9nlnHCJw9fKYDnA7csJnxTrLstvOya9fAq+o64JGVbUmen+SaJDcm+ackP7N6uiTHA8+qqutr+VX6KPCG7al6NI+f5qCqfgAcOs3BSucClw+3rwLOHL5pnAtcWVX/U1X3AXcPz7fmMm1mw+VSVfdX1S3AYztR4NjWec1WvvaXs/b7+zXAwap6pKr+EzgInLVlhc5gk2M8F/hoLfsicPTwmZ90vJMsu22zawN8HQeAt1fVS4F3An+xRp8TWD5I6ZAHhrZDLhq+el2241+f1ncC8I0V91eP4Ql9qupR4DvAsyectqvdPLbNeG5VPQgwXD9njT7dl9V6Y1xvXJOOd5Jlt22OmABP8gzg5cAnktwM/CVw/Fpd12g7tK/lB4HnAy8GHgT+eAtKHcMkpzlYr89Ep0hoajePbWy7dVntqvf9ERPgLI/1v6rqxSsuP5tkz4ofJd/H8l/eE1dM9/gpAqrqoar636p6DPgrhk0LT0KTnObg8T5J9gI/yfLX0N18ioTdPLbNeGjYbHBok+HDa/TpvqzWG+N645p0vJMsu21zxAR4VX0XuC/Jr8Ljv0b/3BDIhwL93cPXou8lOX3YJvxm4FPDNCvX2H8FeLLujTHJaQ6uBg790n4e8Plhm//VwPnDXionAS9g+Ufd3cDTPyxb+dpfyPD+XuWzwKuTHDNsKnz10NbFemO8Gnjz8Pk/HfjO8JmfdLyTLLvts9O/IG/VBbiC5c0cP2T5r+vbWN6r4hqW9z64HXj3OtMusBzO9wAf4P+PWP0b4CvALSy/kMfv9DgPM/6zga8NY7h4aHsf8Prh9tOAT7D8I+WXgJNXTHvxMN2dDHvgrLdMd3qcW7Bcfn4Y2/eB/wBu2+maZxzvWp+DZwPXAncN18cOfReAS1dM+9bh/XE38JadHstIYwzL/4jmnuGzvLDReIFLD/Vb73l36uKh9JLU1BGzCUWSdhsDXJKaMsAlqSkDXJKaMsAlqSkDXJKaMsAlqan/A4hK+dLDwsBNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_q11 = [1 if row[0]==0 else -1 for row in train]\n",
    "y_test_q11 = [1 if row[0]==0 else -1 for row in test]\n",
    "prob  = svm_problem(y_train_q11, x_train)\n",
    "\n",
    "def getWeightNorm(c_str):\n",
    "    \n",
    "    param = svm_parameter(('-t 0 -c '+c_str))\n",
    "    m = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y=[1,1,1,1], x=[[0.,0.,0.], [1.,0.,0.], [0.,1.,0.], [0.,0.,1.]], m=m)\n",
    "    w_q11 = np.array(p_val[1:])\n",
    "    w_q11 = w_q11-p_val[0]\n",
    "    return np.linalg.norm(w_q11)\n",
    "\n",
    "C = [1e-5, 1e-3, 1e-1, 1e+1, 1e+3]\n",
    "C_str = [str(c) for c in C]\n",
    "w_norm = [getWeightNorm(c_str) for c_str in C_str]\n",
    "plt.figure()\n",
    "plt.bar(C_str, w_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "As $C$ getting greater in value, $||w||$ also increase. <br />\n",
    "It makes sense because when $C$ is small, the objective function will mainly focus on minimizing 1/2$||w||^2$, <br />\n",
    "so the optimal $||w||$ would be small.\n",
    "<br /><br /><br /><br /><br /><br /><br /><br />\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 92.5662% (6749/7291) (classification)\n",
      "Accuracy = 92.5662% (6749/7291) (classification)\n",
      "Accuracy = 92.5662% (6749/7291) (classification)\n",
      "Accuracy = 92.5662% (6749/7291) (classification)\n",
      "Accuracy = 92.5662% (6749/7291) (classification)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_q12 = [1 if row[0]==8 else -1 for row in train]\n",
    "y_test_q12 = [1 if row[0]==8 else -1 for row in test]\n",
    "\n",
    "def getEin(c_str):\n",
    "    prob  = svm_problem(y_train_q12, x_train)\n",
    "    param = svm_parameter('-t 1 -d 2 -c '+c_str) #Set parameters\n",
    "    m = svm_train(prob, param) #Training\n",
    "    p_label, p_acc, p_val = svm_predict(y_train_q12, x_train, m) #Predict training set\n",
    "    return (100-p_acc[0])/100 #Calculate E_in\n",
    "\n",
    "C = [1e-5, 1e-3, 1e-1, 1e+1, 1e+3]\n",
    "C_str = [str(c) for c in C]\n",
    "Ein_q12 = [getEin(c_str) for c_str in C_str]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(C_str, Ein_q12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "$E_{in}$ doesn't change with the value of $C$.\n",
    "<br /><br /><br /><br /><br /><br /><br /><br /><br /><br />\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_q12 = [1 if row[0]==8 else -1 for row in train]\n",
    "y_test_q12 = [1 if row[0]==8 else -1 for row in test]\n",
    "\n",
    "def getNumSV(c_str):\n",
    "    prob  = svm_problem(y_train_q12, x_train)\n",
    "    param = svm_parameter('-t 1 -d 2 -c '+c_str) #Set parameters\n",
    "    m = svm_train(prob, param) #Training\n",
    "    return(m.get_nr_sv()) #Get the number of SVs\n",
    "\n",
    "C = [1e-5, 1e-3, 1e-1, 1e+1, 1e+3]\n",
    "C_str = [str(c) for c in C]\n",
    "nSV_q12 = [getNumSV(c_str) for c_str in C_str]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(C_str, nSV_q12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "Similar to $E_{in}$, the number of SV doesn't change much with the value of $C$.\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14\n",
    "<font size=\"4\"> \n",
    "Much thanks to the classmates who discussed this question on the forum. <br />\n",
    "<br />\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "g(x) & = w^Tz+b \\\\\n",
    " & = \\sum_{sv}\\alpha_ny_nK(x_n,x)+b \\\\\n",
    " & =\\sum_{sv}\\alpha_ny_nK(x_n,x)+y_{sv}-\\sum_{sv}\\alpha_ny_nK(x_n,x) \\\\\n",
    " & =y_{sv}\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "<br />\n",
    "<br />\n",
    "Hence, in the Z space, the distance from the hyperplane to a free-SV is <br />\n",
    "$\\frac{1}{||w||}|w^Tz+b|=\\frac{1}{||w||}|g(x)|=\\frac{1}{||w||}$  <br />\n",
    "<br />\n",
    "For calculating $||w||$, we know that $w=\\sum_{sv}\\alpha_iy_iz_i$, and we can have<br />\n",
    "$||w||^2=\\sum_{sv}\\sum_{sv}\\alpha_iy_iz_i\\alpha_jy_jz_j=\\sum_{sv}\\sum_{sv}\\alpha_iy_i\\alpha_jy_jK(x_i,x_j)$\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_q14 = [1 if row[0]==0 else -1 for row in train]\n",
    "y_test_q14 = [1 if row[0]==0 else -1 for row in test]\n",
    "\n",
    "def K_rbf(x1, x2, g=80):\n",
    "    return np.exp(-g*sum((x1-x2)**2))\n",
    "\n",
    "def getFreeSVDist(c_str, y=y_train_q14):\n",
    "    prob  = svm_problem(y, x_train)\n",
    "    param = svm_parameter('-t 2 -g 80 -c '+c_str)\n",
    "    m = svm_train(prob, param)\n",
    "    SV = m.get_SV() #Get SVs\n",
    "    SV = [np.array([sv[1], sv[2]]) for sv in SV] #Extract SV from dictionary to numpy.array\n",
    "    SV_idx = m.get_sv_indices() #Get the indices of SVs in the training data\n",
    "    a = m.get_sv_coef() #Get coeficient a\n",
    "    a = [aa[0] for aa in a] #Extract a from nested list to list\n",
    "    nSV = len(SV) #Get the number of SVs\n",
    "    # Calculate the ||w||^2\n",
    "    w_norm2 = np.sum([a[i]*a[j]*y[SV_idx[i]-1]*y[SV_idx[j]-1]*K_rbf(SV[i], SV[j]) for i in range(nSV) for j in range(nSV)])\n",
    "    return(w_norm2**(-1/2)) #return 1/||w||\n",
    "\n",
    "C = [1e-3, 1e-2, 1e-1, 1e+0, 1e+1]\n",
    "C_str = [str(c) for c in C]\n",
    "dist_q14 = [getFreeSVDist(c_str) for c_str in C_str]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(C_str, dist_q14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "The distance from the hyperplane to a free-SV decreases drastically while $C$ increase. <br />\n",
    "It is probably because that when $C$ is small, the SVM has more tolerence to error terms, so the optimizer can come up with a larger margin.\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.439% (6521/7291) (classification)\n",
      "Accuracy = 90.2208% (6578/7291) (classification)\n",
      "Accuracy = 90.3991% (6591/7291) (classification)\n",
      "Accuracy = 83.6236% (6097/7291) (classification)\n",
      "Accuracy = 83.6236% (6097/7291) (classification)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob  = svm_problem(y_train_q14, x_train)\n",
    "param = svm_parameter('-t 2 -g 80 -c 0.1')\n",
    "m = svm_train(prob, param)\n",
    "\n",
    "def getEout(g_str):\n",
    "    prob  = svm_problem(y_train_q14, x_train)\n",
    "    param = svm_parameter('-t 2 -c 0.1 -g '+g_str) #Ser parameters\n",
    "    m = svm_train(prob, param) #Training\n",
    "    p_label, p_acc, p_val = svm_predict(y_test_q14, x_test, m) #Predict testing set\n",
    "    return (100-p_acc[0])/100 #Calculate E_out\n",
    "\n",
    "G = [1e0, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "G_str = [str(g) for g in G]\n",
    "Eout_q15 = [getEout(g_str) for g_str in G_str]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(G_str, Eout_q15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "The result shows that it is important to find proper $\\gamma$ while testing the model. <br />\n",
    "The model is not always better with the big parameter, and vice versa.\n",
    "<br /><br /><br /><br /><br /><br /><br /><br />\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = np.arange(len(x_train)) #Set indices from sampling\n",
    "x_train = np.array(x_train)\n",
    "y_train_q14 = np.array(y_train_q14)\n",
    "\n",
    "# Function to calculate E_out with assigned gamma value\n",
    "def getEval(g_str, x_train, y_train, x_val, y_val):\n",
    "    prob  = svm_problem(y_train, x_train)\n",
    "    param = svm_parameter('-t 2 -c 0.1 -g '+g_str)\n",
    "    m = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y_val, x_val, m)\n",
    "    return (100-p_acc[0])/100\n",
    "\n",
    "def getBestG():\n",
    "    random.shuffle(idx) #Shuffle the indices\n",
    "    x_val_q16 = x_train[idx[:1000], ] #Get the first 1000 indices for validating set\n",
    "    y_val_q16 = y_train_q14[idx[:1000]] #Get the first 1000 indices for validating set\n",
    "    x_train_q16 = x_train[idx[1000:], ] #Get the rest of the indices for training set\n",
    "    y_train_q16 = y_train_q14[idx[1000:]] # Get the rest of the indices for testing set\n",
    "    \n",
    "    G = [1e-1, 1e0, 1e+1, 1e+2, 1e+3]\n",
    "    G_str = [str(g) for g in G]\n",
    "    #Calculate E_out for each gamma in G_str\n",
    "    Eout_q15 = [getEval(g_str, x_train_q16, y_train_q16, x_val_q16, y_val_q16) for g_str in G_str]\n",
    "    return G_str[np.argmin(Eout_q15)] #Return the gamma with the minimal E_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 83.8% (838/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 83.8% (838/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 91% (910/1000) (classification)\n",
      "Accuracy = 91% (910/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 90.2% (902/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 90.2% (902/1000) (classification)\n",
      "Accuracy = 91% (910/1000) (classification)\n",
      "Accuracy = 90.7% (907/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 82.3% (823/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 82.3% (823/1000) (classification)\n",
      "Accuracy = 85% (850/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 91.4% (914/1000) (classification)\n",
      "Accuracy = 91% (910/1000) (classification)\n",
      "Accuracy = 85% (850/1000) (classification)\n",
      "Accuracy = 82.1% (821/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 87.9% (879/1000) (classification)\n",
      "Accuracy = 82.1% (821/1000) (classification)\n",
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 82.4% (824/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 82.4% (824/1000) (classification)\n",
      "Accuracy = 83.4% (834/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 83.4% (834/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 90.8% (908/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 88.1% (881/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 90.8% (908/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 84.5% (845/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 90.2% (902/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 88.4% (884/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 85.1% (851/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 91% (910/1000) (classification)\n",
      "Accuracy = 91.1% (911/1000) (classification)\n",
      "Accuracy = 85.1% (851/1000) (classification)\n",
      "Accuracy = 83.2% (832/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 83.2% (832/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 90.8% (908/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 84.8% (848/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 90.2% (902/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 84.8% (848/1000) (classification)\n",
      "Accuracy = 81.6% (816/1000) (classification)\n",
      "Accuracy = 87.9% (879/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 88.4% (884/1000) (classification)\n",
      "Accuracy = 81.6% (816/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 88.7% (887/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 82.4% (824/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 82.4% (824/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 84.6% (846/1000) (classification)\n",
      "Accuracy = 85.7% (857/1000) (classification)\n",
      "Accuracy = 90.7% (907/1000) (classification)\n",
      "Accuracy = 91.3% (913/1000) (classification)\n",
      "Accuracy = 91.7% (917/1000) (classification)\n",
      "Accuracy = 85.7% (857/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 85.2% (852/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 86.5% (865/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 86.5% (865/1000) (classification)\n",
      "Accuracy = 84% (840/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 84% (840/1000) (classification)\n",
      "Accuracy = 87% (870/1000) (classification)\n",
      "Accuracy = 91.5% (915/1000) (classification)\n",
      "Accuracy = 91.8% (918/1000) (classification)\n",
      "Accuracy = 92% (920/1000) (classification)\n",
      "Accuracy = 87% (870/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 91.3% (913/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 88.8% (888/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 81% (810/1000) (classification)\n",
      "Accuracy = 87.2% (872/1000) (classification)\n",
      "Accuracy = 87.8% (878/1000) (classification)\n",
      "Accuracy = 87.9% (879/1000) (classification)\n",
      "Accuracy = 81% (810/1000) (classification)\n",
      "Accuracy = 84.8% (848/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 84.8% (848/1000) (classification)\n",
      "Accuracy = 86% (860/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 91.7% (917/1000) (classification)\n",
      "Accuracy = 91.4% (914/1000) (classification)\n",
      "Accuracy = 86% (860/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 91.3% (913/1000) (classification)\n",
      "Accuracy = 91.6% (916/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 84.2% (842/1000) (classification)\n",
      "Accuracy = 88.7% (887/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 84.2% (842/1000) (classification)\n",
      "Accuracy = 81.8% (818/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 81.8% (818/1000) (classification)\n",
      "Accuracy = 85% (850/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 85% (850/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 84.4% (844/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 84.4% (844/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 82.9% (829/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 85.3% (853/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 85.3% (853/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 82.6% (826/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 82.6% (826/1000) (classification)\n",
      "Accuracy = 85.6% (856/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 91.9% (919/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 85.6% (856/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 88.4% (884/1000) (classification)\n",
      "Accuracy = 88.1% (881/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 88.4% (884/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 83.8% (838/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 83.8% (838/1000) (classification)\n",
      "Accuracy = 86.2% (862/1000) (classification)\n",
      "Accuracy = 91.9% (919/1000) (classification)\n",
      "Accuracy = 92.8% (928/1000) (classification)\n",
      "Accuracy = 92.5% (925/1000) (classification)\n",
      "Accuracy = 86.2% (862/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 90.6% (906/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 84.3% (843/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 91.3% (913/1000) (classification)\n",
      "Accuracy = 91.7% (917/1000) (classification)\n",
      "Accuracy = 83.7% (837/1000) (classification)\n",
      "Accuracy = 82.5% (825/1000) (classification)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 88.8% (888/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 82.5% (825/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 91.4% (914/1000) (classification)\n",
      "Accuracy = 91.2% (912/1000) (classification)\n",
      "Accuracy = 84.7% (847/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 88.2% (882/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 83% (830/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 89.4% (894/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 85.3% (853/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 90.9% (909/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 85.3% (853/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 82.1% (821/1000) (classification)\n",
      "Accuracy = 88.1% (881/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 82.1% (821/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 90.7% (907/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.7% (907/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 88.7% (887/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 82% (820/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 82% (820/1000) (classification)\n",
      "Accuracy = 82.3% (823/1000) (classification)\n",
      "Accuracy = 88.3% (883/1000) (classification)\n",
      "Accuracy = 89.2% (892/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 82.3% (823/1000) (classification)\n",
      "Accuracy = 84% (840/1000) (classification)\n",
      "Accuracy = 88.4% (884/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 84% (840/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 88.7% (887/1000) (classification)\n",
      "Accuracy = 84.1% (841/1000) (classification)\n",
      "Accuracy = 85.8% (858/1000) (classification)\n",
      "Accuracy = 91.5% (915/1000) (classification)\n",
      "Accuracy = 91.5% (915/1000) (classification)\n",
      "Accuracy = 91.1% (911/1000) (classification)\n",
      "Accuracy = 85.8% (858/1000) (classification)\n",
      "Accuracy = 85.1% (851/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90.7% (907/1000) (classification)\n",
      "Accuracy = 90.4% (904/1000) (classification)\n",
      "Accuracy = 85.1% (851/1000) (classification)\n",
      "Accuracy = 84.4% (844/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 89.6% (896/1000) (classification)\n",
      "Accuracy = 84.4% (844/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 83.2% (832/1000) (classification)\n",
      "Accuracy = 88.7% (887/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 83.2% (832/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 88.8% (888/1000) (classification)\n",
      "Accuracy = 89.3% (893/1000) (classification)\n",
      "Accuracy = 89.5% (895/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 89.8% (898/1000) (classification)\n",
      "Accuracy = 89.9% (899/1000) (classification)\n",
      "Accuracy = 83.1% (831/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 88.5% (885/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 88.6% (886/1000) (classification)\n",
      "Accuracy = 82.7% (827/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 88.9% (889/1000) (classification)\n",
      "Accuracy = 90% (900/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 83.3% (833/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 89.7% (897/1000) (classification)\n",
      "Accuracy = 90.8% (908/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 83.5% (835/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 90.8% (908/1000) (classification)\n",
      "Accuracy = 90.3% (903/1000) (classification)\n",
      "Accuracy = 83.9% (839/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 89.1% (891/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 89% (890/1000) (classification)\n",
      "Accuracy = 82.8% (828/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 90.5% (905/1000) (classification)\n",
      "Accuracy = 90.1% (901/1000) (classification)\n",
      "Accuracy = 83.6% (836/1000) (classification)\n"
     ]
    }
   ],
   "source": [
    "resBestG = [getBestG() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "resCount = dict(Counter(resBestG)) #Count duplicates of each gamma in the result\n",
    "G = [1e-1, 1e0, 1e+1, 1e+2, 1e+3]\n",
    "G_str = [str(g) for g in G]\n",
    "#Fill out the list of gamma's duplicate\n",
    "G_choice = [resCount.get(g_str) if resCount.get(g_str) else 0 for g_str in G_str] \n",
    "\n",
    "plt.figure()\n",
    "plt.bar(G_str, G_choice)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "From the result of 100 times of vlidations, we can infer that model is suitable with $\\gamma=10$ while $C=0.1$\n",
    "<font />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 17\n",
    "<font size=\"4\"> \n",
    "Let $z_n$ be the transformed data that start we constant 1<br />\n",
    "<br />\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "w & = \\sum_{sv}\\alpha_ny_nz_n \\\\\n",
    " & = \\sum_{sv}\\alpha_ny_n\\phi(x_n) \\\\\n",
    " & = \\sum_{sv}\\alpha_ny_n(1+...) \n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "<br />\n",
    "<br />\n",
    "We can seperate the constant term as $w_i$ <br />\n",
    "and since $\\alpha_{notSV}=0$ <br />\n",
    "<br />\n",
    "$w_i=\\sum_{sv}\\alpha_ny_n(1)=\\sum_{n=1}^{N}\\alpha_ny_n$\n",
    "<br />\n",
    "<br />\n",
    "Since the optimal weight $w$ observes constraint $\\sum_{n=1}^{N}a_ny_n=0$ <br />\n",
    "We can know that $w_i=0$\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
